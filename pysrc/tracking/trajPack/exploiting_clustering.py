import os, pdb, sys, time, operator, getpass
import numpy as np
import cPickle as pickle
from util.listFileManagement import correct_from_Nan
if getpass.getuser()=='aschoenauer':
    import matplotlib as mpl
    mpl.use('Agg')
elif getpass.getuser()=='lalil0u':
    import matplotlib as mpl
    
import matplotlib.pyplot as p
import brewer2mpl

from collections import defaultdict, Counter
from itertools import product
from math import sqrt
from numpy.linalg import inv
from operator import itemgetter
from optparse import OptionParser
from peach import FuzzyCMeans
from sklearn.covariance import MinCovDet
from sklearn.cluster import MiniBatchKMeans, SpectralClustering
from sklearn.mixture import GMM, DPGMM
from sklearn.manifold import Isomap
from scipy.spatial.distance import cdist
from scipy.spatial import KDTree
from scipy.stats import pearsonr, chi2_contingency
from sklearn.utils.graph import graph_laplacian

from clustering import dist
from tracking.trajPack import featuresNumeriques, featuresSaved
from rpy2 import robjects
from rpy2.robjects.packages import importr
rStats=importr("stats")


from tracking.plots import plotClustInd, makeColorRamp, plotMovies, plotKMeansPerFilm, plotPermutationResults
#from joblib import Parallel, delayed, Memory

def exploitingKMeans_wModel(model, data, mean, std, pca_std, pca, num_PC=7):
    data=np.hstack((data[:,:len(featuresNumeriques)], data[:, featuresSaved.index('mean straight'), np.newaxis]))
    pca_data=pca.transform((data-mean)/std)/pca_std
    return np.bincount(model.predict(pca_data[:,:num_PC]), minlength=8)

def exploitingKMeans(data,ctrldata, length,ctrlStatus,k, who, genes,sirna, iteration=10, colors='ctrlStatus',\
                     Td=True, show=False, plotcov=False):          
    if iteration==False:
        model = MiniBatchKMeans(n_clusters=k, batch_size = 2000, init='k-means++',n_init=1000,max_iter=1000, max_no_improvement=100, compute_labels = True)
        zou=model.fit(data)
        labels=zou.labels_
        if ctrldata is not None:
            ctrl_labels=model.predict(ctrldata)
            labels=np.hstack((labels,ctrl_labels))
        
        if show==True:
            percentages = filmCharacterization(labels, length, ctrlStatus, genes,colors, Td, plotcov)
            return labels, percentages
        else:
            return labels
    else:
        first=True; countsTotal=[]; oldPerc=None
        for it in range(iteration):
            print "iteration ", it
            model = MiniBatchKMeans(n_clusters=k, batch_size = 1000, init='random',n_init=1000,max_iter=1000, max_no_improvement=100, compute_labels = True)
            zou=model.fit(data)
            
            counts, percentages = np.array(filmCount(zou.labels_, length))
            if not first:
                index=np.argmax(percentages, axis=0)
                corr={np.argmax(oldPerc[ind]):np.where(ind==index)[0][0] for ind in index}
                print it, corr
                for el in corr:
                    countsTotal[:,el]+=counts[:,corr[el]]
            else:
                countsTotal=counts
                first=False
                oldPerc=percentages
        return countsTotal
    
def percek(k, labels, length):
    r=np.bincount(labels[np.sum(length[:k]):np.sum(length[:k+1])])
    if len(r)==3:
        r=np.hstack((r, [0]))
    elif len(r)==2:
        r=np.hstack((r, [0,0]))
    return np.array(r/float(length[k])*100, dtype=int)

def bincount(labels):
    r = np.bincount(labels)
    if len(r)==3:
        r=np.hstack((r, [0]))
    elif len(r)==2:
        r=np.hstack((r, [0,0]))
    return r

def count(labels, ctrlStatus, who, length):
    ctrllabels=[0,0,0,0]; 
    ctrllabelsPerPlate={}; seenPlates={}; ctrllabelsPerFilm={}
    poslabelsPerFilm={}; postests={}
    phenolabelsPerFilm={};
    for i,el in enumerate(ctrlStatus):
        if el==0:
            ctrllabelsPerFilm[who[i]] = bincount(labels[np.sum(length[:i]):np.sum(length[:i+1])])
            ctrllabels+=ctrllabelsPerFilm[who[i]]
            pl,w=who[i]
            if pl not in seenPlates:
                seenPlates[pl]=[w]
                ctrllabelsPerPlate[pl] =ctrllabelsPerFilm[who[i]]
            else: 
                seenPlates[pl].append(w)
                try:
                    ctrllabelsPerPlate[pl]+=ctrllabelsPerFilm[who[i]]
                except:
                    print 'a'
                    pdb.set_trace()
        elif el==2:
            poslabelsPerFilm[who[i]]=bincount(labels[np.sum(length[:i]):np.sum(length[:i+1])])#percek(i, labels, length)
            
        elif el==1:
            try:
                phenolabelsPerFilm[who[i]]=bincount(labels[np.sum(length[:i]):np.sum(length[:i+1])])#percek(i, labels, length)
            except:
                print 'b'
                pdb.set_trace()
        else:
            raise
    ctrllabels=np.array(ctrllabels, dtype=int)
    return ctrllabels, ctrllabelsPerFilm, ctrllabelsPerPlate, phenolabelsPerFilm

def permutationFromLabels(labels, ctrlStatus, length, who, N=10, nb_perm=1000, sh=False):
    sample = np.random.randint(0, len(who), size=N)
    phenotests=defaultdict(list)
    whoCtrl=['00074_01', '00315_01']

    for i in sample:
        pl,w=who[i]
        if w in whoCtrl:
            np.append(sample, np.random.randint(0, len(who), size=1))
            continue
        print i, pl,w

        pLabelsCour = labels[np.sum(length[:i]):np.sum(length[:i+1])]

        ctrlPl = [(pl,well) for well in whoCtrl]
        cLabelsCour =[]
        for ctrlel in ctrlPl:
            try:
                index = who.index(ctrlel)
            except ValueError:
                continue
            else:
                print ctrlel
                cLabelsCour.extend(labels[np.sum(length[:index]):np.sum(length[:index+1])])
        
        vecLongueurs = [0 for k in range(len(cLabelsCour))]; vecLongueurs.extend([1 for k in range(len(pLabelsCour))])
        cLabelsCour.extend(pLabelsCour)
        for iter_ in range(nb_perm):
            cLabelsCour = np.random.permutation(cLabelsCour)
            phenotests[(pl, w)].append(np.float64(rStats.fisher_test(robjects.IntVector(cLabelsCour), robjects.IntVector(vecLongueurs), workspace=200000000)[0])[0])
    if sh: plotPermutationResults(phenotests)
    return phenotests

def testingFromLabels(labels, ctrlStatus, length, who):
    ctrltests={}; seen=[]
    phenotests={}
    whoCtrl=['00074_01', '00315_01']
    
##ONE SHOULD NOT USE chi-squared test of independency because des cases de la table de contingence sont vides ou <5
##The pbl is that one cannot compare p-values between different tests. So we only use Fisher exact tests.
    zz=0
    for i,el in enumerate(who):
        print i,
        #dealing with experiments
        if ctrlStatus[i]==1:
            pLabelsCour = labels[np.sum(length[:i]):np.sum(length[:i+1])]
            pl=el[0]
            ctrlPl = [(pl,w) for w in whoCtrl]
            cLabelsCour =[]
            for ctrlel in ctrlPl:
                try:
                    index = who.index(ctrlel)
                except ValueError:
                    continue
                else:
                    cLabelsCour.extend(labels[np.sum(length[:index]):np.sum(length[:index+1])])
            
            vecLongueurs = [0 for k in range(len(cLabelsCour))]; vecLongueurs.extend([1 for k in range(len(pLabelsCour))])
            cLabelsCour.extend(pLabelsCour)
            try:
                phenotests[(el[0], el[1])]=np.float64(rStats.fisher_test(robjects.IntVector(cLabelsCour), robjects.IntVector(vecLongueurs), 
                                                                         workspace=200000000, simulate_p_value=True)[0])[0]
            except:
                pdb.set_trace() 
                
        if el[0] not in seen:
        #computing p-values for stat tests between different controls on the same plate
            pl=el[0]
            seen.append(pl)
            ctrlPl = np.array(who)[np.where(np.array(ctrlStatus)==0)]
            ctrlPl = ctrlPl[np.where(ctrlPl[:,0]==pl)]
            print 'il y a {} ctrl sur cette plaque {}'.format(len(ctrlPl), pl)
#TODO a tester
            pdb.set_trace()
            cLabelsCour =[]
            for ctrlel in ctrlPl:
                try:
                    index = who.index(ctrlel)
                except ValueError:
                    continue
                else:
                    cLabelsCour.append(labels[np.sum(length[:index]):np.sum(length[:index+1])])
                    
            if len(cLabelsCour)<2:
                zz+=1
                continue
            vecLongueurs = [0 for k in range(len(cLabelsCour[0]))]
            for i,el in enumerate(ctrlPl[1:]):  
                vecLongueurs.extend([i+1 for k in range(len(cLabelsCour[i+1]))])
                cLabelsCour[0]=np.hstack((cLabelsCour[0], cLabelsCour[i+1]))
            try:
                ctrltests[el[0]]=np.float64(rStats.fisher_test(robjects.IntVector(cLabelsCour[0]), robjects.IntVector(vecLongueurs), 
                                                                         workspace=200000000, simulate_p_value=True)[0])[0]
            except:
                pdb.set_trace()         
    print 'missed', zz
    return ctrltests, phenotests

def BHYconservativeQVals(phenotests):
        
    #computing q-values according to Benjamini-Hochberg procedure for tests with very local dependence
    #which is our case here
    qvals=[]; pvals=sorted(phenotests.values())
    for k in range(len(pvals)):
        qvals.append(pvals[k]*len(pvals)/float(k+1))
        
    return pvals, qvals

def geneHit(seuilp, seuilq, phenotests, who, siRNA, genes, ensemblFichier=None):
    siHit=[]; geneHit=[]; geneValidated=[]; siVal={}; geneHitVal={}; expOK=defaultdict(list)
    siCount=Counter(siRNA)
    pvals, qvals = BHYconservativeQVals(phenotests)
#    pPvals, qPvals=BHYconservativeQVals(phenoPtests)
    maxQVal=10**(-10)
    for exp in phenotests:
        if phenotests[exp]<seuilp and qvals[pvals.index(phenotests[exp])]<seuilq: 
#            if phenoPtests[exp]<seuilp and qPvals[pPvals.index(phenoPtests[exp])]<seuilq:
            expOK[genes[who.index(exp)]].append(exp)
            siHit.append(siRNA[who.index(exp)])
            maxQVal = max(maxQVal, qvals[pvals.index(phenotests[exp])])#, qPvals[pPvals.index(phenoPtests[exp])])
#            if siRNA[who.index(exp)] not in siVal:
#                siVal[siRNA[who.index(exp)]]=[phenotests[exp]]
#            else:
#                siVal[siRNA[who.index(exp)]].append(phenotests[exp])
            
    siHit=Counter(siHit)
    for si in siHit:
        if float(siHit[si])/siCount[si]>=0.5:
            indice=siRNA.index(si)
            gene=genes[indice]
            geneHit.append(gene)
#            if gene not in geneHitVal:
##we record the median because we want that at least half of the pvals have an acceptable FDR
#                geneHitVal[gene]=np.median(siVal[si])
#        #and if there are multiple sirnas we want to have at least half of them have an acceptable FDR
#            else: geneHitVal[gene]=np.median(geneHitVal[gene], np.median(siVal[si]))
    geneHit=Counter(geneHit)
    geneSiAssociation = zip(genes, siRNA)
    
    geneSiCount=Counter(np.array(Counter(geneSiAssociation).keys())[:,0])
    for gene in geneHit:
        if float(geneHit[gene])/geneSiCount[gene]>=0.5:
            geneValidated.append(gene)
    print 'Selecting {} hits, {} validated, max q-value: {}'.format(len(geneHit), len(geneValidated), maxQVal)
    
    if ensemblFichier !=None:
        l = EnsemblEntrezTrad(ensemblFichier)
#FIXME !!!
#        geneHit.remove('unfound'); geneValidated.remove('unfound')
        ensemblH = [l[gene] for gene in geneHit]; ensemblV = [l[gene] for gene in  geneValidated]
        print 'Returning lists in ENSEMBL'
        return expOK, ensemblH, ensemblV
        
    else:
        print 'Returning lists in SYMBOLS'
        return expOK, geneHit, geneValidated#, zip(geneHitVal.keys(), geneHitVal.values())
    
def toGSEA(phenotests, who, siRNA, genes):
    siVal={}; geneHitVal={}; 
    
    for exp in phenotests:
        if siRNA[who.index(exp)] not in siVal:
            siVal[siRNA[who.index(exp)]]=[phenotests[exp]]
        else:
            siVal[siRNA[who.index(exp)]].append(phenotests[exp])
        
    for si in siVal:
        indice=siRNA.index(si)
        gene=genes[indice]
    
        if gene not in geneHitVal:
#HERE WE TAKE THE MEDIAN because we want that it is reproducible
            geneHitVal[gene]=np.median(siVal[si])
        else: geneHitVal[gene]=min(geneHitVal[gene], np.median(siVal[si]))
    zou = zip(geneHitVal.keys(), geneHitVal.values())
    zou.sort(key=itemgetter(1))
    return siVal, zou
    
def exploitingGMixtures(data, length,ctrlStatus, k,covar, genes=None, colors='ctrlStatus', Td=True,plotcov=True, show=False):
    debut_film = time.clock()
    
    model = GMM(n_components=k, covariance_type=covar, random_state=None, thresh=0.01, min_covar=0.001, 
                        n_iter=10, n_init=10, params='wmc', init_params='wmc')
    model.fit(data)
    labels=model.predict(data)

    print 'fin film', time.clock()-debut_film
    if show:
        percentages = filmCharacterization(labels, length, ctrlStatus, genes, colors,Td, plotcov)
        return labels, percentages
    else: return labels, model

def filmCount(labels, length):
    courant = 0
    k=np.bincount(labels).shape[0]
    result = np.zeros(shape=(len(length), k))
    result2 = np.zeros(shape=(len(length), k))
    print "Computing counts per film"
    for l in range(len(length)):
        labels_film = labels[courant : courant + length[l]]
        try:
            result[l]=np.bincount(labels_film)
            result2[l]=np.bincount(labels_film)/float(length[l])*100
        except ValueError:
            print l
            try:
                result[l]=np.hstack((np.bincount(labels_film), [0]))
                result2[l]=np.hstack((np.bincount(labels_film), [0]))/float(length[l])*100
            except ValueError:
                print l
                result[l]=np.hstack((np.bincount(labels_film), [0,0]))
                result2[l]=np.hstack((np.bincount(labels_film), [0,0]))/float(length[l])*100
        courant+=length[l]
    return result, result2

def filmCharacterization(labels, length, ctrlStatus, piclabels, colors, Td=False, plotcov=True, show=True):
    courant = 0
    k=np.bincount(labels).shape[0]
    result = np.zeros(shape=(len(length), k))
    print "Computing percentages per film"
    for l in range(len(length)):
        labels_film = labels[courant : courant + length[l]]
        result[l]=np.bincount(labels_film, minlength=k)/float(length[l])*100
        courant+=length[l]
        
    ctrlcov=None
    if plotcov:
        print "Estimating covariance of ctrl movies in the space of cluster percentages"
        ctrlpercentages=[]
        for i,el in enumerate(ctrlStatus):
            if el==0:
                ctrlpercentages.append(result[i])
        ctrlpercentages=np.array(ctrlpercentages)
        ctrlcov=MinCovDet().fit(ctrlpercentages[:,:2])
    if show:
        piclabels2=['{} {}'.format(piclabels[k], length[k]) for k in range(len(piclabels))]
        plotKMeansPerFilm(result, ctrlStatus,piclabels2, colors, Td, ctrlcov)
    
    return result

def permutation(labels, lastLabels, k):
    intersection =np.zeros(shape=(k,k))
    labels=np.array(labels); lastLabels = np.array(lastLabels)
    for i in range(k):
        for j in range(k):
            intersection[i,j]=len(filter(lambda x: x in np.where(lastLabels==i)[0] and x in np.where(labels==j)[0], range(len(labels))))
    #ordonner : argsort sur un array
    #corresp pour les nouveaux labels
#    pdb.set_trace()
    return np.argmax(intersection, 0)

def outputBin(data, ctrlSize,nbPheno, lPheno, binSize, sigma, nbDim=2, nbNeighbours=20):
    m = Isomap(n_neighbors=nbNeighbours, n_components=nbDim, eigen_solver='auto', tol=0, max_iter=None, path_method='auto', neighbors_algorithm='kd_tree')
    D = m.fit_transform(data)
    ctrl = D[:ctrlSize]
    ctrlTree = KDTree(ctrl, leafsize=10)
    length=ctrlSize
    
    mini = np.amin(D, 0); maxi=np.amax(D, 0); 
    nbPointsX = int((maxi[0]-mini[0])/float(binSize))+1
    nbPointsY = int((maxi[1]-mini[1])/float(binSize))+1
    
    result = np.zeros(shape=(nbPheno, nbPointsX, nbPointsY))
    denomCtrl = np.zeros(shape=(nbPointsX, nbPointsY))
    
    for pointX, pointY in product(range(nbPointsX), range(nbPointsY)):
        x=mini[0]+(pointX+0.5)*binSize; y=mini[1]+(pointY+0.5)*binSize
        ctrldou, ctrli = ctrlTree.query((x, y), ctrlSize, distance_upper_bound=binSize/sqrt(2))
        if min(ctrldou)<100:
            ctrlPoint = filter(lambda t: t[1]<ctrl.shape[0] and np.all(np.abs(ctrl[t[1]]-(x, y))<(binSize/2.0, binSize/2.0)), zip(ctrldou, ctrli))        
            for distance, cPoint in ctrlPoint:
                denomCtrl[pointX, pointY]+=dist((x,y), ctrl[cPoint], sigma)
                
    for ifilm in range(nbPheno):
        print 'film ', ifilm
        pheno = D[length:length+lPheno[ifilm]]
        phenoTree = KDTree(pheno, leafsize=10)
        
        for pointX, pointY in product(range(nbPointsX), range(nbPointsY)):
            x=mini[0]+(pointX+0.5)*binSize; y=mini[1]+(pointY+0.5)*binSize
            denom=denomCtrl[pointX, pointY]
            phenodou, phenoi=phenoTree.query((x, y), data.shape[0]-ctrlSize, distance_upper_bound=binSize/sqrt(2))
            if min(phenodou)<100:
                phenoPoint =filter(lambda t: t[1]<pheno.shape[0] and np.all(np.abs(pheno[t[1]]-(x, y))<(binSize/2.0, binSize/2.0)), zip(phenodou, phenoi))
                for distance, pPoint in phenoPoint:
                    local = dist((x,y), pheno[pPoint], sigma)
                    result[ifilm, pointX, pointY]+=local; denom+=local
        length+=lPheno[ifilm]        
        if denom>0:result[ifilm, pointX, pointY]/=denom
    plotMovies('/media/lalil0u/New/workspace2/Tracking/images', result, 'pattern_b{}_s{}'.format(binSize, sigma))
    return result


def testStabilite(filename,inFolder, k, div_name, dist_weights, seuil=0.05, 
                  bins_type="minmax", bin_size=10, iterations=10, cost_type='number', 
                  outFolder=None, save=False, plot=True):
    #loading other data than numeric and histogram data
    file_ ='{}OtherData.pkl'.format(filename)
    who,ctrlStatus, length, genes, sirna=pickle.load(file_)
    f.close()
    
    cResult=[]
    hits = []; validated=[]
    
    #then to load output from clustering
    filename+='_a1_k{}_d{}_w{}_bt{}_bs{}_ct{}_{}.pkl'
    if type(k)==int:
        for iteration in range(iterations):
            f=open(os.path.join(inFolder, filename.format(k,div_name[:5],dist_weights,bins_type[:3], bin_size,cost_type[:3], iteration)), 'r')
            _, labels, _=pickle.load(f); f.close()
            ctrltests, phenotests = testingFromLabels(labels, ctrlStatus, length, who)
            arr = np.array(ctrltests.keys())
            cResult.append(len(arr[np.where(arr<seuil)]))
            print 'nb de controles sous la barre ', len(arr[np.where(arr<seuil)]) 
            _, geneHit, geneValidated = geneHit(seuil,seuil, phenotests, who, sirna, genes)
            hits.extend(geneHit); validated.extend(geneValidated)
        
        hits = Counter(hits)
        hits.update({g:0 for g in filter(lambda x: x not in hits.keys(), genes)})
        validated = Counter(validated)
        validated.update({g:0 for g in filter(lambda x: x not in validated.keys(), genes)})

        return hits, validated
    elif type(k)==list:
        #ie je cherche a calculer la stabilite des listes de hit et de validated genes pour des nombres de cluster differents
        
        result = {}
        for num in k:
            result[num] = testStabilite(filename, inFolder, num, div_name, dist_weights, seuil, bins_type, bin_size, iterations, cost_type, outFolder, False)
            
        if save:
            outFolder = inFolder if outFolder is None else outFolder
            f=open(os.path.join(outFolder, 'stabResults_ki{}_kf{}_p{}_{}_{}_w{}_bs{}_bt{}_.pkl'.format
                                (k[0], k[-1], seuil, div_name[:5], dist_weights, bin_size, bins_type, iterations)))
            pickle.dump(result, f); f.close()
        if plot:
            plotStabilite(result, genes, iterations, hit=True)
            plotStabilite(result, genes, iterations, hit=False)
        return

def plotStabilite(dic, genes, iterations, hit=True):
    #dic est un dictionnaire qui contient {nb de clusters : evaluation de la stabilite sur 10 iterations}
    emplacement = 0 if hit else 1
    arr = np.zeros(shape=(len(dic.keys(), len(genes))), dtype=float)
    for i,gene in enumerate(genes):
        for num in dic:
            arr[num, i]=dic[num][emplacement][gene]/float(iterations)
            
    fig = p.figure(figsize=(12,7))
    ax=fig.add_subplot(111)
    cmap = brewer2mpl.get_map('RdPu', 'Sequential', 9).mpl_colormap

    ax.pcolormesh(arr, cmap=cmap, edgecolors = 'black')
    p.show()  

if __name__ == '__main__':
    description =\
'''
%prog - Parallel clustering
You can in particular set up the noise level
'''
    parser = OptionParser(usage="usage: %prog [options]",
                         description=description)

    parser.add_option("-p", dest="plate",
                      help="The plate which you are interested in")
    parser.add_option("-w", dest="well",
                      help="The well which you are interested in")
    
    (options, args) = parser.parse_args()

    print "Working on ", options.plate, options.well

    model_file='../resultData/features_on_films/kmeans_model.pkl'
    in_folder = '/share/data20T/mitocheck/tracking_results'
    out_folder = '../resultData/features_on_films/trajectory_cluster_distributions'
    try:
        f=open(os.path.join(in_folder, options.plate, 'hist_tabFeatures_{}.pkl'.format(options.well)))
        arr, _, _= pickle.load(f); f.close()
    except:
        labels=None
    else:
        arr, _ = correct_from_Nan(arr, perMovie=False)
        
        f=open(model_file, 'r')
        model, mean, std, pca, pca_std=pickle.load(f)
        
        labels=exploitingKMeans_wModel(model, arr, mean, std, pca_std, pca, num_PC=7)
        
    finally:
        try:
            f=open(os.path.join(out_folder, 'trajectory_cluster_{}.pkl'.format(options.plate)))
            d=pickle.load(f)
            f.close()
        except:
            d={}
        finally:
            d.update({options.well:labels})
            f=open(os.path.join(out_folder, 'trajectory_cluster_{}.pkl'.format(options.plate)), 'w')
            pickle.dump(d,f)
            f.close()
            
        print "Finished"
    





