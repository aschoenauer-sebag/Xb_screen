import os, pdb, sys, time, operator, getpass
import numpy as np
import cPickle as pickle
if getpass.getuser()=='aschoenauer':
    import matplotlib as mpl
    mpl.use('Agg')
elif getpass.getuser()=='lalil0u':
    import matplotlib as mpl
    
import matplotlib.pyplot as p
import brewer2mpl

from collections import defaultdict, Counter
from itertools import product
from math import sqrt
from numpy.linalg import inv
from operator import itemgetter
from optparse import OptionParser
from peach import FuzzyCMeans
from sklearn.covariance import MinCovDet
from sklearn.cluster import MiniBatchKMeans, SpectralClustering
from sklearn.mixture import GMM, DPGMM
from sklearn.manifold import Isomap
from scipy.spatial.distance import cdist
from scipy.spatial import KDTree
from scipy.stats import pearsonr, chi2_contingency
from sklearn.utils.graph import graph_laplacian

from clustering import dist
from util.listFileManagement import txtToList
from tracking.trajPack import sdFeatures1, logTrsf
from rpy2 import robjects
from rpy2.robjects.packages import importr
rStats=importr("stats")


from tracking.plots import plotClustInd, makeColorRamp, plotMovies, plotKMeansPerFilm, plotPermutationResults
#from joblib import Parallel, delayed, Memory

def exploitingKMeans(data,ctrldata, length,ctrlStatus,k, who, genes,sirna, iteration=10, colors='ctrlStatus',\
                     Td=True, show=False, plotcov=False):          
    if iteration==False:
        model = MiniBatchKMeans(n_clusters=k, batch_size = 2000, init='k-means++',n_init=1000,max_iter=1000, max_no_improvement=100, compute_labels = True)
        zou=model.fit(data)
        labels=zou.labels_
        ctrl_labels=model.predict(ctrldata)
        labels=np.hstack((labels,ctrl_labels))
        
        if show==True:
            percentages = filmCharacterization(labels, length, ctrlStatus, genes,colors, Td, plotcov)
            return labels, percentages
        else:
            return labels
    else:
#        exp, val, hit=defaultdict(list),[], []
        first=True; countsTotal=[]; oldPerc=None
        for it in range(iteration):
            print "iteration ", it
            model = MiniBatchKMeans(n_clusters=k, batch_size = 1000, init='random',n_init=1000,max_iter=1000, max_no_improvement=100, compute_labels = True)
            zou=model.fit(data)
            
            counts, percentages = np.array(filmCount(zou.labels_, length))
#            for i,el in enumerate(percentages):                                                                                                        
#                    if np.any(el>70):
#                        print el, who[i], genes[i], length[i]

            if not first:
                index=np.argmax(percentages, axis=0)
                corr={np.argmax(oldPerc[ind]):np.where(ind==index)[0][0] for ind in index}
                print it, corr
                for el in corr:
                    countsTotal[:,el]+=counts[:,corr[el]]
            else:
                countsTotal=counts
                first=False
                oldPerc=percentages
        return countsTotal
#        ctest, cptest, ptest, pptest = testingFromCounts(countsTotal, ctrlStatus, length, who)
#        ok, valc, hitc= geneHit(0.05, 0.05, ptest, pptest, who, sirna, genes)

#        return ok,valc, hitc#{gene:set(filter(lambda x: Counter(exp[gene])[x]>7,exp[gene])) for gene in filter(lambda x: Counter(hit)[gene]>7, Counter(hit))}, Counter(val), Counter(hit)
    
    
def percek(k, labels, length):
    r=np.bincount(labels[np.sum(length[:k]):np.sum(length[:k+1])])
    if len(r)==3:
        r=np.hstack((r, [0]))
    elif len(r)==2:
        r=np.hstack((r, [0,0]))
    return np.array(r/float(length[k])*100, dtype=int)

def bincount(labels):
    r = np.bincount(labels)
    if len(r)==3:
        r=np.hstack((r, [0]))
    elif len(r)==2:
        r=np.hstack((r, [0,0]))
    return r

def count(labels, ctrlStatus, who, length):
    ctrllabels=[0,0,0,0]; 
    ctrllabelsPerPlate={}; seenPlates={}; ctrllabelsPerFilm={}
    poslabelsPerFilm={}; postests={}
    phenolabelsPerFilm={};
    for i,el in enumerate(ctrlStatus):
        if el==0:
            ctrllabelsPerFilm[who[i]] = bincount(labels[np.sum(length[:i]):np.sum(length[:i+1])])
            ctrllabels+=ctrllabelsPerFilm[who[i]]
            pl,w=who[i]
            if pl not in seenPlates:
                seenPlates[pl]=[w]
                ctrllabelsPerPlate[pl] =ctrllabelsPerFilm[who[i]]
            else: 
                seenPlates[pl].append(w)
                try:
                    ctrllabelsPerPlate[pl]+=ctrllabelsPerFilm[who[i]]
                except:
                    print 'a'
                    pdb.set_trace()
        elif el==2:
            poslabelsPerFilm[who[i]]=bincount(labels[np.sum(length[:i]):np.sum(length[:i+1])])#percek(i, labels, length)
            
        elif el==1:
            try:
                phenolabelsPerFilm[who[i]]=bincount(labels[np.sum(length[:i]):np.sum(length[:i+1])])#percek(i, labels, length)
            except:
                print 'b'
                pdb.set_trace()
        else:
            raise
    ctrllabels=np.array(ctrllabels, dtype=int)
    return ctrllabels, ctrllabelsPerFilm, ctrllabelsPerPlate, phenolabelsPerFilm

#def testingFromCounts(counts, ctrlStatus, length, who):
#    #que faire avec les pourcentages ? et les labels
#    ctrltests={}; ctrlPtests={}
#    phenotests={}; phenoPtests={}
#    ctrllabels=[0,0,0,0];
#    ctrllabelsPerPlate={}; 
#    ctrllabelsPerFilm={}; seenPlates={};
#    phenolabelsPerFilm={};
#    
#    for i,el in enumerate(who):
#        if ctrlStatus[i]==0:
#            pl,w=who[i]
#            ctrllabels+=counts[i]
#            ctrllabelsPerFilm[who[i]]=counts[i]
#            if pl not in ctrllabelsPerPlate:
#                ctrllabelsPerPlate[pl]=counts[i]
#            else:
#                ctrllabelsPerPlate[pl]+=counts[i]
#        elif ctrlStatus[i]==1:
#            phenolabelsPerFilm[who[i]]=counts[i]
#            
#    countfish=0
#    cproportions=np.array(ctrllabels/float(np.sum(ctrllabels))*100, dtype=int)
###ONE SHOULD NOT USE chi-squared test of independency because des cases de la table de contingence sont vides ou <5
#
#    for pl,w in ctrllabelsPerFilm:        
#        prop=np.array(ctrllabelsPerFilm[(pl,w)]/float(np.sum(ctrllabelsPerFilm[(pl,w)]))*100, dtype=int)              
#        if np.all(np.vstack((ctrllabelsPerFilm[(pl,w)], ctrllabels))>5):
#            ctrltests[(pl,w)]=chi2_contingency(np.vstack((ctrllabelsPerFilm[(pl,w)], ctrllabels)))[1]
#        else:
#            print 'no ctrl fisher cause causes crash dump'
#            ctrltests[(pl,w)]=0
#            
#        if np.all(np.vstack((prop, cproportions))>5):
#            ctrlPtests[(pl,w)]=chi2_contingency(np.vstack((prop, cproportions)))[1]
#        else:
#            #print 'ctrl prop fisher'
#            l=list(prop)+list(cproportions)
#            ctrlPtests[(pl,w)]=ftest(int(l[0]), int(l[1]), int(l[2]), int(l[3]), int(l[4]), int(l[5]), int(l[6]), int(l[7]))
#            
##    for pl,w in poslabelsPerFilm:
##        print 'pos'
##        pass
#    for pl,w in phenolabelsPerFilm:
#        prop = np.array(phenolabelsPerFilm[(pl,w)]/float(np.sum(phenolabelsPerFilm[(pl,w)]))*100, dtype=int)
#        cprop = np.array(ctrllabelsPerPlate[pl]/float(np.sum(ctrllabelsPerPlate[pl]))*100, dtype=int)
#        if np.all(np.vstack((phenolabelsPerFilm[(pl,w)], ctrllabelsPerPlate[pl]))>5):
#            phenotests[(pl,w)]=chi2_contingency(np.vstack((phenolabelsPerFilm[(pl,w)], ctrllabelsPerPlate[pl])))[1]
#        else:
#            countfish+=1
##            
##            if np.sum(l)>1000:
##                continue
##            else:
#            l=list(phenolabelsPerFilm[(pl,w)])+list(ctrllabelsPerPlate[pl])
#            print pl, 'pheno fisher',l, np.sum(l)
#            if l>4000:
#                phenotests[(pl,w)]=0.0001
#            else:
#                phenotests[(pl,w)]=ftest(int(l[0]), int(l[1]), int(l[2]), int(l[3]), int(l[4]), int(l[5]), int(l[6]), int(l[7]))
#            
#        if np.all(np.vstack((prop, cprop))>5):
#            phenoPtests[(pl,w)]=chi2_contingency(np.vstack((prop, cprop)))[1]
#        else:
#            l=list(prop)+list(cprop)
##            print 'pheno prop fisher', l
#            phenoPtests[(pl, w)]=ftest(int(l[0]), int(l[1]), int(l[2]), int(l[3]), int(l[4]), int(l[5]), int(l[6]), int(l[7]))
#            #print l, phenoPtests[(pl, w)]
#        #
#    print "Number of Fisher exact tests", countfish
#    return ctrltests, ctrlPtests, phenotests, phenoPtests

def permutationFromLabels(labels, ctrlStatus, length, who, N=10, nb_perm=1000, sh=False):
    sample = np.random.randint(0, len(who), size=N)
    phenotests=defaultdict(list)
    whoCtrl=['00074_01', '00315_01']

    for i in sample:
        pl,w=who[i]
        if w in whoCtrl:
            np.append(sample, np.random.randint(0, len(who), size=1))
            continue
        print i, pl,w

        pLabelsCour = labels[np.sum(length[:i]):np.sum(length[:i+1])]

        ctrlPl = [(pl,well) for well in whoCtrl]
        cLabelsCour =[]
        for ctrlel in ctrlPl:
            try:
                index = who.index(ctrlel)
            except ValueError:
                continue
            else:
                print ctrlel
                cLabelsCour.extend(labels[np.sum(length[:index]):np.sum(length[:index+1])])
        
        vecLongueurs = [0 for k in range(len(cLabelsCour))]; vecLongueurs.extend([1 for k in range(len(pLabelsCour))])
        cLabelsCour.extend(pLabelsCour)
        for iter_ in range(nb_perm):
            cLabelsCour = np.random.permutation(cLabelsCour)
            phenotests[(pl, w)].append(np.float64(rStats.fisher_test(robjects.IntVector(cLabelsCour), robjects.IntVector(vecLongueurs), workspace=200000000)[0])[0])
    if sh: plotPermutationResults(phenotests)
    return phenotests

def testingFromLabels(labels, ctrlStatus, length, who):
    ctrltests={}; seen=[]
    phenotests={}
    whoCtrl=['00074_01', '00315_01']
    
##ONE SHOULD NOT USE chi-squared test of independency because des cases de la table de contingence sont vides ou <5
##The pbl is that one cannot compare p-values between different tests. So we only use Fisher exact tests.
    zz=0
    for i,el in enumerate(who):
        print i,
        #dealing with experiments
        if ctrlStatus[i]==1:
            pLabelsCour = labels[np.sum(length[:i]):np.sum(length[:i+1])]
            pl=el[0]
            ctrlPl = [(pl,w) for w in whoCtrl]
            cLabelsCour =[]
            for ctrlel in ctrlPl:
                try:
                    index = who.index(ctrlel)
                except ValueError:
                    continue
                else:
                    cLabelsCour.extend(labels[np.sum(length[:index]):np.sum(length[:index+1])])
            
            vecLongueurs = [0 for k in range(len(cLabelsCour))]; vecLongueurs.extend([1 for k in range(len(pLabelsCour))])
            cLabelsCour.extend(pLabelsCour)
            try:
                phenotests[(el[0], el[1])]=np.float64(rStats.fisher_test(robjects.IntVector(cLabelsCour), robjects.IntVector(vecLongueurs), 
                                                                         workspace=200000000, simulate_p_value=True)[0])[0]
            except:
                pdb.set_trace() 
                
        if el[0] not in seen:
        #computing p-values for stat tests between different controls on the same plate
            pl=el[0]
            seen.append(pl)
            ctrlPl = np.array(who)[np.where(np.array(ctrlStatus)==0)]
            ctrlPl = ctrlPl[np.where(ctrlPl[:,0]==pl)]
            print 'il y a {} ctrl sur cette plaque {}'.format(len(ctrlPl), pl)
#TODO a tester
            pdb.set_trace()
            cLabelsCour =[]
            for ctrlel in ctrlPl:
                try:
                    index = who.index(ctrlel)
                except ValueError:
                    continue
                else:
                    cLabelsCour.append(labels[np.sum(length[:index]):np.sum(length[:index+1])])
                    
            if len(cLabelsCour)<2:
                zz+=1
                continue
            vecLongueurs = [0 for k in range(len(cLabelsCour[0]))]
            for i,el in enumerate(ctrlPl[1:]):  
                vecLongueurs.extend([i+1 for k in range(len(cLabelsCour[i+1]))])
                cLabelsCour[0]=np.hstack((cLabelsCour[0], cLabelsCour[i+1]))
            try:
                ctrltests[el[0]]=np.float64(rStats.fisher_test(robjects.IntVector(cLabelsCour[0]), robjects.IntVector(vecLongueurs), 
                                                                         workspace=200000000, simulate_p_value=True)[0])[0]
            except:
                pdb.set_trace()         
    print 'missed', zz
    return ctrltests, phenotests

def BHYconservativeQVals(phenotests):
    #computing extremely conservative q-values according to Benjamini-Hochberg-Yekutieli procedure for 
    #tests with no particular independence
#    qvals=[]; pvals=sorted(phenotests.values())
#    for k in range(len(pvals)):
#        qvals.append(pvals[k]*len(pvals)*np.log(len(pvals))/float(k+1))
        
    #computing q-values according to Benjamini-Hochberg procedure for tests with very local dependence
    #which is our case here
    qvals=[]; pvals=sorted(phenotests.values())
    for k in range(len(pvals)):
        qvals.append(pvals[k]*len(pvals)/float(k+1))
        
    return pvals, qvals

def geneHit(seuilp, seuilq, phenotests, who, siRNA, genes, ensemblFichier=None):
    siHit=[]; geneHit=[]; geneValidated=[]; siVal={}; geneHitVal={}; expOK=defaultdict(list)
    siCount=Counter(siRNA)
    pvals, qvals = BHYconservativeQVals(phenotests)
#    pPvals, qPvals=BHYconservativeQVals(phenoPtests)
    maxQVal=10**(-10)
    for exp in phenotests:
        if phenotests[exp]<seuilp and qvals[pvals.index(phenotests[exp])]<seuilq: 
#            if phenoPtests[exp]<seuilp and qPvals[pPvals.index(phenoPtests[exp])]<seuilq:
            expOK[genes[who.index(exp)]].append(exp)
            siHit.append(siRNA[who.index(exp)])
            maxQVal = max(maxQVal, qvals[pvals.index(phenotests[exp])])#, qPvals[pPvals.index(phenoPtests[exp])])
#            if siRNA[who.index(exp)] not in siVal:
#                siVal[siRNA[who.index(exp)]]=[phenotests[exp]]
#            else:
#                siVal[siRNA[who.index(exp)]].append(phenotests[exp])
            
    siHit=Counter(siHit)
    for si in siHit:
        if float(siHit[si])/siCount[si]>=0.5:
            indice=siRNA.index(si)
            gene=genes[indice]
            geneHit.append(gene)
#            if gene not in geneHitVal:
##we record the median because we want that at least half of the pvals have an acceptable FDR
#                geneHitVal[gene]=np.median(siVal[si])
#        #and if there are multiple sirnas we want to have at least half of them have an acceptable FDR
#            else: geneHitVal[gene]=np.median(geneHitVal[gene], np.median(siVal[si]))
    geneHit=Counter(geneHit)
    geneSiAssociation = zip(genes, siRNA)
    
    geneSiCount=Counter(np.array(Counter(geneSiAssociation).keys())[:,0])
    for gene in geneHit:
        if float(geneHit[gene])/geneSiCount[gene]>=0.5:
            geneValidated.append(gene)
    print 'Selecting {} hits, {} validated, max q-value: {}'.format(len(geneHit), len(geneValidated), maxQVal)
    
    if ensemblFichier !=None:
        l = EnsemblEntrezTrad(ensemblFichier)
#FIXME !!!
#        geneHit.remove('unfound'); geneValidated.remove('unfound')
        ensemblH = [l[gene] for gene in geneHit]; ensemblV = [l[gene] for gene in  geneValidated]
        print 'Returning lists in ENSEMBL'
        return expOK, ensemblH, ensemblV
        
    else:
        print 'Returning lists in SYMBOLS'
        return expOK, geneHit, geneValidated#, zip(geneHitVal.keys(), geneHitVal.values())
    
def toGSEA(phenotests, who, siRNA, genes):
    siVal={}; geneHitVal={}; 
    
    for exp in phenotests:
        if siRNA[who.index(exp)] not in siVal:
            siVal[siRNA[who.index(exp)]]=[phenotests[exp]]
        else:
            siVal[siRNA[who.index(exp)]].append(phenotests[exp])
        
    for si in siVal:
        indice=siRNA.index(si)
        gene=genes[indice]
    
        if gene not in geneHitVal:
#HERE WE TAKE THE MEDIAN because we want that it is reproducible
            geneHitVal[gene]=np.median(siVal[si])
        else: geneHitVal[gene]=min(geneHitVal[gene], np.median(siVal[si]))
    zou = zip(geneHitVal.keys(), geneHitVal.values())
    zou.sort(key=itemgetter(1))
    return siVal, zou
    
def exploitingGMixtures(data, length,ctrlStatus, k,covar, genes=None, colors='ctrlStatus', Td=True,plotcov=True, show=False):
    debut_film = time.clock()
    
    model = GMM(n_components=k, covariance_type=covar, random_state=None, thresh=0.01, min_covar=0.001, 
                        n_iter=10, n_init=10, params='wmc', init_params='wmc')
    model.fit(data)
    labels=model.predict(data)

    print 'fin film', time.clock()-debut_film
    if show:
        percentages = filmCharacterization(labels, length, ctrlStatus, genes, colors,Td, plotcov)
        return labels, percentages
    else: return labels, model

def filmCount(labels, length):
    courant = 0
    k=np.bincount(labels).shape[0]
    result = np.zeros(shape=(len(length), k))
    result2 = np.zeros(shape=(len(length), k))
    print "Computing counts per film"
    for l in range(len(length)):
        labels_film = labels[courant : courant + length[l]]
        try:
            result[l]=np.bincount(labels_film)
            result2[l]=np.bincount(labels_film)/float(length[l])*100
        except ValueError:
            print l
            try:
                result[l]=np.hstack((np.bincount(labels_film), [0]))
                result2[l]=np.hstack((np.bincount(labels_film), [0]))/float(length[l])*100
            except ValueError:
                print l
                result[l]=np.hstack((np.bincount(labels_film), [0,0]))
                result2[l]=np.hstack((np.bincount(labels_film), [0,0]))/float(length[l])*100
        courant+=length[l]
    return result, result2

def filmCharacterization(labels, length, ctrlStatus, piclabels, colors, Td=False, plotcov=True, show=True):
    courant = 0
    k=np.bincount(labels).shape[0]
    result = np.zeros(shape=(len(length), k))
    print "Computing percentages per film"
    for l in range(len(length)):
        labels_film = labels[courant : courant + length[l]]
        result[l]=np.bincount(labels_film, minlength=k)/float(length[l])*100
        courant+=length[l]
        
    ctrlcov=None
    if plotcov:
        print "Estimating covariance of ctrl movies in the space of cluster percentages"
        ctrlpercentages=[]
        for i,el in enumerate(ctrlStatus):
            if el==0:
                ctrlpercentages.append(result[i])
        ctrlpercentages=np.array(ctrlpercentages)
        ctrlcov=MinCovDet().fit(ctrlpercentages[:,:2])
    if show:
        plotKMeansPerFilm(result, ctrlStatus,piclabels, colors, Td, ctrlcov)
    
    return result

#def exploitingGMixtures(data, length,ctrlStatus, k,covar,choice='vote', batch_size=10000, nb_iter=10, genes=None, colors=None, show=False):
#    ctrl= concatCtrl(data, ctrlStatus, length, len(length)-np.sum(ctrlStatus))
#    pheno = concatCtrl(data, 1-np.array(ctrlStatus), length, np.sum(ctrlStatus))  
#    labels=np.zeros(shape=(np.sum(length),))
#    
#    temp_prop = np.zeros(shape=(5, 10,k))
#    for num_film in (0,1,5,6,7):#(len(length)):
#        debut_film = time.clock()
#        print "film", num_film, ctrlStatus[num_film]
#        dataFilm=data[np.sum(length[:num_film]):np.sum(length[:num_film+1])]
#        percentages = np.zeros(shape=(nb_iter, k))
#        lastLabels = None
##        centers = []; weights = np.zeros(shape=(nb_iter, k))
#        iter_result = np.zeros(shape=(nb_iter, length[num_film]), dtype=int)
#        iter_result_prob = np.zeros(shape=(nb_iter, length[num_film]))
#        for it in range(nb_iter):
#            print 'iteration', it
#            np.random.shuffle(ctrl); np.random.shuffle(pheno)
#            tailleCtrl = batch_size*0.54; taillePheno=batch_size-(length[num_film]+tailleCtrl)
#            dataCtrl = ctrl[:tailleCtrl]; dataPheno=pheno[:taillePheno]
#            
#            data_cour = np.vstack((dataFilm, dataCtrl, dataPheno))
#            print data_cour.shape
#            
#            model = GMM(n_components=k, covariance_type=covar, random_state=None, thresh=0.01, min_covar=0.001, 
#                                n_iter=10, n_init=10, params='wmc', init_params='wmc')
#            model.fit(data_cour)
##            centers.append(model.means_); weights[it]=model.weights_
##ENCORE UNE QUESTION EN SUSPENS, EST-CE QU'ON VEUT PREDIRE SUR TOUT LE MONDE OU SEULEMENT LE FILM D'INTERET ?
#            labels=model.predict(data_cour)
#            #probas = model.predict_proba(data_cour)
##            
##            if it>0:
##                permut_labels = permutation(labels[:length[num_film]], iter_result[0],k)
##                
##                indices = []
##                for i in range(k):
##                    indices.append(np.where(labels==i))
##                for i in range(k):
##                    labels[indices[i]]=permut_labels[i]
##
##            #lastLabels = list(labels)
##            
##            iter_result[it]=labels[:length[num_film]]
##            percentages[it] = np.bincount(labels[:length[num_film]])/float(length[num_film])*100
##        if choice=='vote':
##            #ici pour chaque trajectoire je dis la repartition dans les clusters
##            bincounts = [np.bincount(np.array(iter_result[:,p], dtype=int)) for p in range(iter_result.shape[1])]
##            labels[np.sum(length[:num_film]):np.sum(length[:num_film+1])]=[np.argmax(bincounts[p]) for p in range(iter_result.shape[1])]
##        elif choice=='mean':
###FIXME TODO
##            pass
##        temp_prop[num_film]=percentages
#    print 'fin film', time.clock()-debut_film
#    
#           # labels[np.sum(length[:k]):np.sum(length[:k+1])]=np.mean(iter)
##    courant = 0
##    result = np.zeros(shape=(len(length), k))
##    for k in range(len(length)):
##        labels_film = labels[courant : courant + length[k]]
##        result[k]=np.bincount(labels_film)/float(length[k])*100
##        courant+=length[k]
##    if show:plotKMeansPerFilm(result, ctrlStatus, genes, colors)
##    
#    return temp_prop

def permutation(labels, lastLabels, k):
    intersection =np.zeros(shape=(k,k))
    labels=np.array(labels); lastLabels = np.array(lastLabels)
    for i in range(k):
        for j in range(k):
            intersection[i,j]=len(filter(lambda x: x in np.where(lastLabels==i)[0] and x in np.where(labels==j)[0], range(len(labels))))
    #ordonner : argsort sur un array
    #corresp pour les nouveaux labels
#    pdb.set_trace()
    return np.argmax(intersection, 0)

def outputBin(data, ctrlSize,nbPheno, lPheno, binSize, sigma, nbDim=2, nbNeighbours=20):
    m = Isomap(n_neighbors=nbNeighbours, n_components=nbDim, eigen_solver='auto', tol=0, max_iter=None, path_method='auto', neighbors_algorithm='kd_tree')
    D = m.fit_transform(data)
    ctrl = D[:ctrlSize]
    ctrlTree = KDTree(ctrl, leafsize=10)
    length=ctrlSize
    
    mini = np.amin(D, 0); maxi=np.amax(D, 0); 
    nbPointsX = int((maxi[0]-mini[0])/float(binSize))+1
    nbPointsY = int((maxi[1]-mini[1])/float(binSize))+1
    
    result = np.zeros(shape=(nbPheno, nbPointsX, nbPointsY))
    denomCtrl = np.zeros(shape=(nbPointsX, nbPointsY))
    
    for pointX, pointY in product(range(nbPointsX), range(nbPointsY)):
        x=mini[0]+(pointX+0.5)*binSize; y=mini[1]+(pointY+0.5)*binSize
        ctrldou, ctrli = ctrlTree.query((x, y), ctrlSize, distance_upper_bound=binSize/sqrt(2))
        if min(ctrldou)<100:
            ctrlPoint = filter(lambda t: t[1]<ctrl.shape[0] and np.all(np.abs(ctrl[t[1]]-(x, y))<(binSize/2.0, binSize/2.0)), zip(ctrldou, ctrli))        
            for distance, cPoint in ctrlPoint:
                denomCtrl[pointX, pointY]+=dist((x,y), ctrl[cPoint], sigma)
                
    for ifilm in range(nbPheno):
        print 'film ', ifilm
        pheno = D[length:length+lPheno[ifilm]]
        phenoTree = KDTree(pheno, leafsize=10)
        
        for pointX, pointY in product(range(nbPointsX), range(nbPointsY)):
            x=mini[0]+(pointX+0.5)*binSize; y=mini[1]+(pointY+0.5)*binSize
            denom=denomCtrl[pointX, pointY]
            phenodou, phenoi=phenoTree.query((x, y), data.shape[0]-ctrlSize, distance_upper_bound=binSize/sqrt(2))
            if min(phenodou)<100:
                phenoPoint =filter(lambda t: t[1]<pheno.shape[0] and np.all(np.abs(pheno[t[1]]-(x, y))<(binSize/2.0, binSize/2.0)), zip(phenodou, phenoi))
                for distance, pPoint in phenoPoint:
                    local = dist((x,y), pheno[pPoint], sigma)
                    result[ifilm, pointX, pointY]+=local; denom+=local
        length+=lPheno[ifilm]        
        if denom>0:result[ifilm, pointX, pointY]/=denom
    plotMovies('/media/lalil0u/New/workspace2/Tracking/images', result, 'pattern_b{}_s{}'.format(binSize, sigma))
    return result


def testStabilite(filename,inFolder, k, div_name, dist_weights, seuil=0.05, 
                  bins_type="minmax", bin_size=10, iterations=10, cost_type='number', 
                  outFolder=None, save=False, plot=True):
    #loading other data than numeric and histogram data
    file_ ='{}OtherData.pkl'.format(filename)
    who,ctrlStatus, length, genes, sirna=pickle.load(file_)
    f.close()
    
    cResult=[]
    hits = []; validated=[]
    
    #then to load output from clustering
    filename+='_a1_k{}_d{}_w{}_bt{}_bs{}_ct{}_{}.pkl'
    if type(k)==int:
        for iteration in range(iterations):
            f=open(os.path.join(inFolder, filename.format(k,div_name[:5],dist_weights,bins_type[:3], bin_size,cost_type[:3], iteration)), 'r')
            _, labels, _=pickle.load(f); f.close()
            ctrltests, phenotests = testingFromLabels(labels, ctrlStatus, length, who)
            arr = np.array(ctrltests.keys())
            cResult.append(len(arr[np.where(arr<seuil)]))
            print 'nb de controles sous la barre ', len(arr[np.where(arr<seuil)]) 
            _, geneHit, geneValidated = geneHit(seuil,seuil, phenotests, who, sirna, genes)
            hits.extend(geneHit); validated.extend(geneValidated)
        
        hits = Counter(hits)
        hits.update({g:0 for g in filter(lambda x: x not in hits.keys(), genes)})
        validated = Counter(validated)
        validated.update({g:0 for g in filter(lambda x: x not in validated.keys(), genes)})

        return hits, validated
    elif type(k)==list:
        #ie je cherche a calculer la stabilite des listes de hit et de validated genes pour des nombres de cluster differents
        
        result = {}
        for num in k:
            result[num] = testStabilite(filename, inFolder, num, div_name, dist_weights, seuil, bins_type, bin_size, iterations, cost_type, outFolder, False)
            
        if save:
            outFolder = inFolder if outFolder is None else outFolder
            f=open(os.path.join(outFolder, 'stabResults_ki{}_kf{}_p{}_{}_{}_w{}_bs{}_bt{}_.pkl'.format
                                (k[0], k[-1], seuil, div_name[:5], dist_weights, bin_size, bins_type, iterations)))
            pickle.dump(result, f); f.close()
        if plot:
            plotStabilite(result, genes, iterations, hit=True)
            plotStabilite(result, genes, iterations, hit=False)
        return

def plotStabilite(dic, genes, iterations, hit=True):
    #dic est un dictionnaire qui contient {nb de clusters : evaluation de la stabilite sur 10 iterations}
    emplacement = 0 if hit else 1
    arr = np.zeros(shape=(len(dic.keys(), len(genes))), dtype=float)
    for i,gene in enumerate(genes):
        for num in dic:
            arr[num, i]=dic[num][emplacement][gene]/float(iterations)
            
    fig = p.figure(figsize=(12,7))
    ax=fig.add_subplot(111)
    cmap = brewer2mpl.get_map('RdPu', 'Sequential', 9).mpl_colormap

    ax.pcolormesh(arr, cmap=cmap, edgecolors = 'black')
    p.show()
    
#    if result ==None:
#        result=[]
#        for iter_ in range(iterations):
#            labels = exploitingKMeans(data, length, ctrlStatus, k, who, genes, sirna, iteration=False)
#            _,_, tests, _=testingFromLabels(labels, ctrlStatus, length, who)
#            result.append(tests)
#            print iter_
#    ids=result[0].keys()
#    zou=np.zeros(shape=(len(ids), iterations))
#    for i,el in enumerate(ids):
#        for iter_ in range(iterations):
#            zou[i][iter_]=result[iter_][el]    
#    correlations=[]
#    coeffs=[]
#    for iter_ in range(1,iterations):
#        corr,_=pearsonr(zou[:,0], zou[:,iter_]); correlations.append(corr)
#        x=zou[:,0]; y=zou[:,iter_]; res=zip(x,y)
#        res=sorted(res, key=operator.itemgetter(0))
#        a, _, _, _ = np.linalg.lstsq(x[:,np.newaxis], y)
#        coeffs.append(a)
#    #plot
#    
#    
#    return correlations, coeffs
#    

if __name__ == '__main__':
    description =\
'''
%prog - Parallel clustering
You can in particular set up the noise level
'''
    parser = OptionParser(usage="usage: %prog [options]",
                         description=description)

    parser.add_option("--file", dest="file",
                      help="The file which you are interested in")
    parser.add_option("--nbparts", dest="nb", type=int,default=0,
                      help="Nb of data files")
    parser.add_option("--normed", dest="normed", type=int,default=0,
                      help="Data already normed or not")
    
    parser.add_option("-a", "--algo", dest="algo",type=int, 
                      help="1 for Gaussian mixtures, 3 for mini batch KMeans")
    parser.add_option("--folder", dest="outputFolder", default='/cbio/donnees/aschoenauer/workspace2/Tracking/src/',
                      help="output folder")
    parser.add_option("-k", dest="k",type=int, 
                      help="Nb of clusters")    
    parser.add_option("--covariance", dest="covar",type=str, default = 'full', 
                      help="Covariance type for GMM algo")
    parser.add_option("--fuzzy", dest="fuzzifier",type=int, default = 2, 
                      help="Fuzzifier parameter for Fuzzy CMeans algo")

    (options, args) = parser.parse_args()
    debut = time.clock()
#    folder = '/cbio/donnees/aschoenauer/data/tracking/migration'
# PUIS features with densities: folder = '/share/data/mitocheck/tracking_results'
#    allData, whoIsInData, length = concatenation(folder)
    names = dict(zip(range(6), ['isomap', 'gaussianM', 'kMeans', 'batchKMeans', 'SpecClust', 'cMeans']))
    #baseNames = dict(zip(range(4), ['', 'd_p_trsfRR_', 'd_p_trsfRG_0.5out_', 'd_p_trsf_'])); baseName = 'dmax{}'.format(int(options.dens))+baseNames[int(options.data)]
#FOR DMAX50<5
    f_size = len(sdFeatures1)
    s = 120000; first=True
    if options.nb==0:
        f=open(options.file, 'r')
        data=pickle.load(f)
        r=data[0]
        f.close()
        if not bool(options.normed):
            print "normalizing data"
            r=r[:,:20]
            r=(r-np.mean(r, 0))/np.std(r, 0)
        length=None; ctrlStatus=None; genes=None; mot="Il n'y a pas de mot dans ce fichier"
    else:
        for k in range(options.nb):
            filename = '{}{}.pkl'.format(options.file,k+1)
            f=open(filename, 'r') 
            if first:
                r, who, ctrlStatus, length, genes, mot=pickle.load(f)
                first=False
            else:
                r2, who2, ctrlStatus2, length2, genes2, mot=pickle.load(f)
                r=np.vstack((r,r2))
                who.extend(who2); ctrlStatus.extend(ctrlStatus2); length.extend(length2); genes.extend(genes2)
            f.close()
    
    print "Working with such data: ", mot, r.shape 
    
    if options.algo==1:
        labels, model = exploitingGMixtures(r, length, ctrlStatus, options.k, options.covar, genes, colors='gene')
        f=open(os.path.join(options.outputFolder, '{}labels{}.pkl'.format(names[options.algo], r.shape[0])), 'w')
        print 'saving there', os.path.join(options.outputFolder, '{}labels{}.pkl'.format(names[options.algo], r.shape[0]))
        pickle.dump([labels, model], f); f.close()
    elif options.algo==3:
        labels=exploitingKMeans(r, length, ctrlStatus, options.k)
        f=open(os.path.join(options.outputFolder, '{}labels{}.pkl'.format(names[options.algo], r.shape[0])), 'w')
        pickle.dump(labels, f); f.close()
    print "FIN", time.clock()-debut
    
